#!/usr/bin/env python3
"""
æ•´åˆæµ‹è¯•è¿è¡Œå™¨
ç»Ÿä¸€æ‰€æœ‰æµ‹è¯•åŠŸèƒ½çš„å…¥å£ç‚¹
"""
import sys
import asyncio
import subprocess
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils.monitoring_utils import get_comprehensive_status, system_monitor

class IntegratedTestRunner:
    """æ•´åˆæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.results = {}
        self.start_time = None
        self.end_time = None
        
        # æµ‹è¯•å¥—ä»¶å®šä¹‰
        self.test_suites = {
            'quick': [
                ('config_migration_validator.py', 'é…ç½®éªŒè¯', 30),
                ('basic_monitoring_test.py', 'åŸºç¡€ç›‘æ§', 60)
            ],
            'full': [
                ('config_migration_validator.py', 'é…ç½®éªŒè¯', 30),
                ('basic_monitoring_test.py', 'åŸºç¡€ç›‘æ§', 60),
                ('performance_benchmark.py', 'æ€§èƒ½åŸºå‡†', 120),
                ('cache_performance_test.py', 'ç¼“å­˜æ€§èƒ½', 90)
            ],
            'performance': [
                ('performance_benchmark.py', 'æ€§èƒ½åŸºå‡†', 120),
                ('cache_performance_test.py', 'ç¼“å­˜æ€§èƒ½', 90)
            ],
            'monitoring': [
                ('basic_monitoring_test.py', 'åŸºç¡€ç›‘æ§', 60)
            ]
        }
    
    def run_test_script(self, script_name: str, description: str, timeout: int = 60) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•è„šæœ¬"""
        print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {description}")
        print("-" * 50)
        
        script_path = project_root / "scripts" / script_name
        
        if not script_path.exists():
            return {
                'status': 'failed',
                'error': f'æµ‹è¯•è„šæœ¬ä¸å­˜åœ¨: {script_path}',
                'execution_time': 0,
                'output': ''
            }
        
        start_time = time.time()
        
        try:
            # è¿è¡Œæµ‹è¯•è„šæœ¬
            result = subprocess.run(
                [sys.executable, str(script_path)],
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            execution_time = time.time() - start_time
            
            if result.returncode == 0:
                print("âœ… æµ‹è¯•é€šè¿‡")
                return {
                    'status': 'passed',
                    'execution_time': execution_time,
                    'output': result.stdout,
                    'stderr': result.stderr
                }
            else:
                print("âŒ æµ‹è¯•å¤±è´¥")
                return {
                    'status': 'failed',
                    'execution_time': execution_time,
                    'output': result.stdout,
                    'stderr': result.stderr,
                    'return_code': result.returncode
                }
                
        except subprocess.TimeoutExpired:
            execution_time = time.time() - start_time
            print("â° æµ‹è¯•è¶…æ—¶")
            return {
                'status': 'timeout',
                'execution_time': execution_time,
                'error': f'æµ‹è¯•æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)'
            }
        except Exception as e:
            execution_time = time.time() - start_time
            print(f"ğŸ’¥ æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
            return {
                'status': 'error',
                'execution_time': execution_time,
                'error': str(e)
            }
    
    def run_test_suite(self, suite_name: str = 'quick') -> bool:
        """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
        if suite_name not in self.test_suites:
            print(f"âŒ æœªçŸ¥çš„æµ‹è¯•å¥—ä»¶: {suite_name}")
            print(f"å¯ç”¨å¥—ä»¶: {', '.join(self.test_suites.keys())}")
            return False
        
        print(f"ğŸš€ å¼€å§‹è¿è¡Œæµ‹è¯•å¥—ä»¶: {suite_name}")
        print("=" * 60)
        
        self.start_time = datetime.now()
        
        # è·å–ç³»ç»ŸçŠ¶æ€
        pre_test_status = get_comprehensive_status()
        
        # è¿è¡Œæµ‹è¯•
        test_suite = self.test_suites[suite_name]
        for script_name, description, timeout in test_suite:
            test_result = self.run_test_script(script_name, description, timeout)
            self.results[script_name] = {
                'description': description,
                'result': test_result,
                'timestamp': datetime.now().isoformat()
            }
        
        self.end_time = datetime.now()
        
        # è·å–æµ‹è¯•åç³»ç»ŸçŠ¶æ€
        post_test_status = get_comprehensive_status()
        
        # ç”ŸæˆæŠ¥å‘Š
        success = self.generate_test_report(suite_name, pre_test_status, post_test_status)
        
        return success
    
    def generate_test_report(self, suite_name: str, pre_status: Dict, post_status: Dict) -> bool:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        print("=" * 60)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_tests = len(self.results)
        passed_tests = len([r for r in self.results.values() if r['result']['status'] == 'passed'])
        failed_tests = len([r for r in self.results.values() if r['result']['status'] == 'failed'])
        error_tests = len([r for r in self.results.values() if r['result']['status'] == 'error'])
        timeout_tests = len([r for r in self.results.values() if r['result']['status'] == 'timeout'])
        
        total_execution_time = sum(r['result']['execution_time'] for r in self.results.values())
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if passed_tests == total_tests:
            overall_status = 'ğŸ‰ å…¨éƒ¨é€šè¿‡'
        elif passed_tests >= total_tests * 0.8:
            overall_status = 'âš ï¸ å¤§éƒ¨åˆ†é€šè¿‡'
        else:
            overall_status = 'âŒ éœ€è¦ä¿®å¤'
        
        # ç³»ç»Ÿèµ„æºå¯¹æ¯”
        resource_comparison = self.compare_system_resources(pre_status, post_status)
        
        # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        report = {
            'test_suite_info': {
                'suite_name': suite_name,
                'start_time': self.start_time.isoformat(),
                'end_time': self.end_time.isoformat(),
                'total_duration_seconds': (self.end_time - self.start_time).total_seconds(),
                'total_execution_time_seconds': total_execution_time
            },
            'summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': failed_tests,
                'error_tests': error_tests,
                'timeout_tests': timeout_tests,
                'success_rate_percent': success_rate,
                'overall_status': overall_status
            },
            'system_impact': resource_comparison,
            'detailed_results': self.results,
            'pre_test_system_status': pre_status,
            'post_test_system_status': post_status
        }
        
        # ä¿å­˜æŠ¥å‘Š
        logs_dir = project_root / "logs"
        logs_dir.mkdir(exist_ok=True)
        report_path = logs_dir / f"integrated_test_report_{suite_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ˜¾ç¤ºæ‘˜è¦
        print(f"ğŸ“‹ æµ‹è¯•å¥—ä»¶æ‘˜è¦ ({suite_name}):")
        print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"  é€šè¿‡: {passed_tests}")
        print(f"  å¤±è´¥: {failed_tests}")
        print(f"  é”™è¯¯: {error_tests}")
        print(f"  è¶…æ—¶: {timeout_tests}")
        print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  æ€»è€—æ—¶: {total_execution_time:.2f}ç§’")
        print(f"  æ•´ä½“çŠ¶æ€: {overall_status}")
        
        # æ˜¾ç¤ºç³»ç»Ÿå½±å“
        if resource_comparison['significant_changes']:
            print(f"\nâš ï¸ ç³»ç»Ÿèµ„æºå˜åŒ–:")
            for change in resource_comparison['changes']:
                print(f"  - {change}")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print(f"\nğŸ“ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for script_name, test_info in self.results.items():
            status = test_info['result']['status']
            execution_time = test_info['result']['execution_time']
            
            status_icon = {
                'passed': 'âœ…',
                'failed': 'âŒ',
                'error': 'ğŸ’¥',
                'timeout': 'â°'
            }.get(status, 'â“')
            
            print(f"  {status_icon} {test_info['description']}: {status} ({execution_time:.2f}s)")
            
            if status != 'passed' and 'error' in test_info['result']:
                print(f"    é”™è¯¯: {test_info['result']['error']}")
        
        return passed_tests == total_tests
    
    def compare_system_resources(self, pre_status: Dict, post_status: Dict) -> Dict[str, Any]:
        """æ¯”è¾ƒæµ‹è¯•å‰åçš„ç³»ç»Ÿèµ„æº"""
        comparison = {
            'significant_changes': False,
            'changes': []
        }
        
        try:
            pre_resources = pre_status.get('resource_usage', {})
            post_resources = post_status.get('resource_usage', {})
            
            # æ¯”è¾ƒå†…å­˜ä½¿ç”¨
            if 'memory' in pre_resources and 'memory' in post_resources:
                memory_diff = post_resources['memory']['percent'] - pre_resources['memory']['percent']
                if abs(memory_diff) > 5:  # è¶…è¿‡5%å˜åŒ–
                    comparison['changes'].append(f"å†…å­˜ä½¿ç”¨å˜åŒ–: {memory_diff:+.1f}%")
                    comparison['significant_changes'] = True
            
            # æ¯”è¾ƒCPUä½¿ç”¨
            if 'cpu' in pre_resources and 'cpu' in post_resources:
                cpu_diff = post_resources['cpu']['percent'] - pre_resources['cpu']['percent']
                if abs(cpu_diff) > 10:  # è¶…è¿‡10%å˜åŒ–
                    comparison['changes'].append(f"CPUä½¿ç”¨å˜åŒ–: {cpu_diff:+.1f}%")
                    comparison['significant_changes'] = True
            
        except Exception as e:
            comparison['error'] = str(e)
        
        return comparison
    
    def list_available_tests(self):
        """åˆ—å‡ºå¯ç”¨çš„æµ‹è¯•"""
        print("ğŸ“‹ å¯ç”¨çš„æµ‹è¯•å¥—ä»¶:")
        for suite_name, tests in self.test_suites.items():
            print(f"\nğŸ”§ {suite_name}:")
            for script_name, description, timeout in tests:
                print(f"  - {description} ({script_name}, è¶…æ—¶: {timeout}s)")
    
    def run_health_check(self) -> bool:
        """è¿è¡Œå¥åº·æ£€æŸ¥"""
        print("ğŸ¥ è¿è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
        
        try:
            status = get_comprehensive_status()
            health = status.get('health_check', {})
            
            print(f"  ç³»ç»ŸçŠ¶æ€: {health.get('status', 'unknown')}")
            
            if health.get('alerts'):
                print("  âš ï¸ å‘Šè­¦:")
                for alert in health['alerts']:
                    print(f"    - {alert}")
            
            if health.get('warnings'):
                print("  ğŸ’¡ è­¦å‘Š:")
                for warning in health['warnings']:
                    print(f"    - {warning}")
            
            return health.get('status') in ['healthy', 'warning']
            
        except Exception as e:
            print(f"  âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•´åˆæµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument('--suite', '-s', default='quick', 
                       help='æµ‹è¯•å¥—ä»¶ (quick, full, performance, monitoring)')
    parser.add_argument('--list', '-l', action='store_true', 
                       help='åˆ—å‡ºå¯ç”¨çš„æµ‹è¯•å¥—ä»¶')
    parser.add_argument('--health', action='store_true', 
                       help='è¿è¡Œå¥åº·æ£€æŸ¥')
    
    args = parser.parse_args()
    
    runner = IntegratedTestRunner()
    
    if args.list:
        runner.list_available_tests()
        return
    
    if args.health:
        success = runner.run_health_check()
        sys.exit(0 if success else 1)
    
    # è¿è¡Œæµ‹è¯•å¥—ä»¶
    success = runner.run_test_suite(args.suite)
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
