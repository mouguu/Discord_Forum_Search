#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æµ‹è¯•Discordæœºå™¨äººçš„å„é¡¹æ€§èƒ½æŒ‡æ ‡
"""
import sys
import asyncio
import time
import json
import statistics
import random
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.settings import settings

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = {}
        self.baseline_targets = {
            'search_response_time_ms': 2000,  # æœç´¢å“åº”æ—¶é—´ < 2ç§’
            'cache_hit_rate_percent': 85,     # ç¼“å­˜å‘½ä¸­ç‡ > 85%
            'concurrent_searches': 5,         # å¹¶å‘æœç´¢æ•° >= 5
            'memory_efficiency_mb_per_1k_items': 50,  # å†…å­˜æ•ˆç‡ < 50MB/1000é¡¹
            'startup_time_seconds': 10        # å¯åŠ¨æ—¶é—´ < 10ç§’
        }
    
    async def test_search_performance(self, num_searches: int = 100):
        """æµ‹è¯•æœç´¢åŠŸèƒ½æ€§èƒ½"""
        print(f"ğŸ” æµ‹è¯•æœç´¢æ€§èƒ½ ({num_searches} æ¬¡æ¨¡æ‹Ÿæœç´¢)...")
        
        # æ¨¡æ‹Ÿæœç´¢æ“ä½œçš„æ€§èƒ½æµ‹è¯•
        search_times = []
        
        for i in range(num_searches):
            start_time = time.time()
            
            # æ¨¡æ‹Ÿæœç´¢æ“ä½œ
            await self._simulate_search_operation()
            
            search_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            search_times.append(search_time)
            
            # æ¯10æ¬¡æœç´¢æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10 == 0:
                print(f"  è¿›åº¦: {i + 1}/{num_searches}")
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        avg_time = statistics.mean(search_times)
        median_time = statistics.median(search_times)
        min_time = min(search_times)
        max_time = max(search_times)
        p95_time = statistics.quantiles(search_times, n=20)[18]  # 95th percentile
        
        # æ€§èƒ½è¯„ä¼°
        performance_grade = self._grade_performance(avg_time, self.baseline_targets['search_response_time_ms'])
        
        self.results['search_performance'] = {
            'num_searches': num_searches,
            'avg_time_ms': avg_time,
            'median_time_ms': median_time,
            'min_time_ms': min_time,
            'max_time_ms': max_time,
            'p95_time_ms': p95_time,
            'target_time_ms': self.baseline_targets['search_response_time_ms'],
            'performance_grade': performance_grade,
            'meets_target': avg_time <= self.baseline_targets['search_response_time_ms']
        }
        
        print(f"  âœ… å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ms")
        print(f"  âœ… ä¸­ä½æ•°å“åº”æ—¶é—´: {median_time:.2f}ms")
        print(f"  âœ… 95%åˆ†ä½æ•°: {p95_time:.2f}ms")
        print(f"  âœ… æ€§èƒ½ç­‰çº§: {performance_grade}")
        print(f"  âœ… è¾¾åˆ°ç›®æ ‡: {'æ˜¯' if avg_time <= self.baseline_targets['search_response_time_ms'] else 'å¦'}")
        
        return avg_time <= self.baseline_targets['search_response_time_ms']
    
    async def _simulate_search_operation(self):
        """æ¨¡æ‹Ÿæœç´¢æ“ä½œ"""
        # æ¨¡æ‹Ÿæœç´¢æŸ¥è¯¢è§£æ
        await asyncio.sleep(0.001)  # 1ms
        
        # æ¨¡æ‹Ÿæ•°æ®åº“/ç¼“å­˜æŸ¥è¯¢
        await asyncio.sleep(random.uniform(0.005, 0.020))  # 5-20ms
        
        # æ¨¡æ‹Ÿç»“æœå¤„ç†
        await asyncio.sleep(0.002)  # 2ms
        
        # æ¨¡æ‹Ÿå“åº”æ„å»º
        await asyncio.sleep(0.001)  # 1ms
    
    async def test_concurrent_performance(self, concurrent_count: int = 10):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        print(f"âš¡ æµ‹è¯•å¹¶å‘æ€§èƒ½ ({concurrent_count} ä¸ªå¹¶å‘ä»»åŠ¡)...")
        
        start_time = time.time()
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for i in range(concurrent_count):
            task = asyncio.create_task(self._simulate_concurrent_operation(i))
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        successful_tasks = len([r for r in results if not isinstance(r, Exception)])
        failed_tasks = len([r for r in results if isinstance(r, Exception)])
        
        # è®¡ç®—ååé‡
        throughput = successful_tasks / total_time if total_time > 0 else 0
        
        # æ€§èƒ½è¯„ä¼°
        target_concurrent = self.baseline_targets['concurrent_searches']
        performance_grade = self._grade_performance(successful_tasks, target_concurrent, higher_is_better=True)
        
        self.results['concurrent_performance'] = {
            'concurrent_count': concurrent_count,
            'successful_tasks': successful_tasks,
            'failed_tasks': failed_tasks,
            'total_time_seconds': total_time,
            'throughput_tasks_per_second': throughput,
            'target_concurrent': target_concurrent,
            'performance_grade': performance_grade,
            'meets_target': successful_tasks >= target_concurrent
        }
        
        print(f"  âœ… æˆåŠŸä»»åŠ¡: {successful_tasks}/{concurrent_count}")
        print(f"  âœ… æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"  âœ… ååé‡: {throughput:.2f} ä»»åŠ¡/ç§’")
        print(f"  âœ… æ€§èƒ½ç­‰çº§: {performance_grade}")
        
        return successful_tasks >= target_concurrent
    
    async def _simulate_concurrent_operation(self, task_id: int):
        """æ¨¡æ‹Ÿå¹¶å‘æ“ä½œ"""
        # æ¨¡æ‹Ÿä¸åŒçš„å¤„ç†æ—¶é—´
        processing_time = random.uniform(0.1, 0.5)  # 100-500ms
        await asyncio.sleep(processing_time)
        
        # æ¨¡æ‹Ÿå¶å°”çš„å¤±è´¥
        if random.random() < 0.05:  # 5% å¤±è´¥ç‡
            raise Exception(f"æ¨¡æ‹Ÿä»»åŠ¡ {task_id} å¤±è´¥")
        
        return f"ä»»åŠ¡ {task_id} å®Œæˆ"
    
    def test_memory_efficiency(self, num_items: int = 10000):
        """æµ‹è¯•å†…å­˜æ•ˆç‡"""
        print(f"ğŸ’¾ æµ‹è¯•å†…å­˜æ•ˆç‡ ({num_items} ä¸ªæ•°æ®é¡¹)...")
        
        import gc
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # è·å–åˆå§‹å†…å­˜ä½¿ç”¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        initial_objects = len(gc.get_objects())
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = {}
        for i in range(num_items):
            test_data[f"key_{i}"] = {
                'id': i,
                'content': f"test_content_{i}" * 10,  # çº¦150å­—èŠ‚
                'metadata': {
                    'created': datetime.now().isoformat(),
                    'type': 'test',
                    'size': 150
                }
            }
        
        # è·å–æœ€ç»ˆå†…å­˜ä½¿ç”¨
        final_objects = len(gc.get_objects())
        objects_created = final_objects - initial_objects
        
        # ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        estimated_memory_mb = (objects_created * 200) / (1024 * 1024)  # å‡è®¾æ¯ä¸ªå¯¹è±¡200å­—èŠ‚
        memory_per_1k_items = (estimated_memory_mb / num_items) * 1000
        
        # æ€§èƒ½è¯„ä¼°
        target_memory = self.baseline_targets['memory_efficiency_mb_per_1k_items']
        performance_grade = self._grade_performance(memory_per_1k_items, target_memory)
        
        self.results['memory_efficiency'] = {
            'num_items': num_items,
            'objects_created': objects_created,
            'estimated_memory_mb': estimated_memory_mb,
            'memory_per_1k_items_mb': memory_per_1k_items,
            'target_memory_per_1k_mb': target_memory,
            'performance_grade': performance_grade,
            'meets_target': memory_per_1k_items <= target_memory
        }
        
        print(f"  âœ… åˆ›å»ºå¯¹è±¡æ•°: {objects_created:,}")
        print(f"  âœ… ä¼°ç®—å†…å­˜ä½¿ç”¨: {estimated_memory_mb:.2f} MB")
        print(f"  âœ… æ¯1000é¡¹å†…å­˜: {memory_per_1k_items:.2f} MB")
        print(f"  âœ… æ€§èƒ½ç­‰çº§: {performance_grade}")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        del test_data
        gc.collect()
        
        return memory_per_1k_items <= target_memory
    
    def test_startup_performance(self):
        """æµ‹è¯•å¯åŠ¨æ€§èƒ½ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        print("ğŸš€ æµ‹è¯•å¯åŠ¨æ€§èƒ½...")
        
        startup_components = [
            ('é…ç½®åŠ è½½', 0.1),
            ('æ—¥å¿—åˆå§‹åŒ–', 0.05),
            ('ç¼“å­˜åˆå§‹åŒ–', 0.2),
            ('æ•°æ®åº“è¿æ¥', 0.15),
            ('Discordè¿æ¥', 0.3),
            ('æ‰©å±•åŠ è½½', 0.1),
            ('å‘½ä»¤æ³¨å†Œ', 0.05)
        ]
        
        total_startup_time = 0
        component_times = {}
        
        for component, base_time in startup_components:
            # æ¨¡æ‹Ÿå¯åŠ¨æ—¶é—´ï¼ˆæ·»åŠ ä¸€äº›éšæœºæ€§ï¼‰
            component_time = base_time + random.uniform(-0.02, 0.05)
            total_startup_time += component_time
            component_times[component] = component_time
            print(f"  ğŸ“¦ {component}: {component_time:.3f}ç§’")
        
        # æ€§èƒ½è¯„ä¼°
        target_startup = self.baseline_targets['startup_time_seconds']
        performance_grade = self._grade_performance(total_startup_time, target_startup)
        
        self.results['startup_performance'] = {
            'total_startup_time_seconds': total_startup_time,
            'component_times': component_times,
            'target_startup_time_seconds': target_startup,
            'performance_grade': performance_grade,
            'meets_target': total_startup_time <= target_startup
        }
        
        print(f"  âœ… æ€»å¯åŠ¨æ—¶é—´: {total_startup_time:.3f}ç§’")
        print(f"  âœ… æ€§èƒ½ç­‰çº§: {performance_grade}")
        
        return total_startup_time <= target_startup
    
    def test_configuration_performance(self):
        """æµ‹è¯•é…ç½®ç³»ç»Ÿæ€§èƒ½"""
        print("âš™ï¸ æµ‹è¯•é…ç½®ç³»ç»Ÿæ€§èƒ½...")
        
        # æµ‹è¯•é…ç½®è®¿é—®é€Ÿåº¦
        config_access_times = []
        
        for _ in range(1000):
            start_time = time.time()
            
            # è®¿é—®å„ç§é…ç½®é¡¹
            _ = settings.bot.command_prefix
            _ = settings.search.max_messages_per_search
            _ = settings.cache.use_redis
            _ = settings.database.use_database_index
            
            access_time = (time.time() - start_time) * 1000000  # å¾®ç§’
            config_access_times.append(access_time)
        
        avg_access_time = statistics.mean(config_access_times)
        
        # æµ‹è¯•é…ç½®éªŒè¯æ€§èƒ½
        start_time = time.time()
        validation_result = settings.validate()
        validation_time = (time.time() - start_time) * 1000  # æ¯«ç§’
        
        self.results['configuration_performance'] = {
            'avg_config_access_time_microseconds': avg_access_time,
            'config_validation_time_ms': validation_time,
            'validation_passed': validation_result,
            'performance_grade': 'A' if avg_access_time < 10 else 'B' if avg_access_time < 50 else 'C'
        }
        
        print(f"  âœ… å¹³å‡é…ç½®è®¿é—®æ—¶é—´: {avg_access_time:.2f}Î¼s")
        print(f"  âœ… é…ç½®éªŒè¯æ—¶é—´: {validation_time:.2f}ms")
        print(f"  âœ… é…ç½®éªŒè¯ç»“æœ: {'é€šè¿‡' if validation_result else 'å¤±è´¥'}")
        
        return validation_result and avg_access_time < 50  # 50å¾®ç§’å†…
    
    def _grade_performance(self, actual: float, target: float, higher_is_better: bool = False) -> str:
        """æ€§èƒ½ç­‰çº§è¯„ä¼°"""
        if higher_is_better:
            ratio = actual / target if target > 0 else 0
            if ratio >= 1.0:
                return 'A'
            elif ratio >= 0.8:
                return 'B'
            elif ratio >= 0.6:
                return 'C'
            else:
                return 'D'
        else:
            ratio = actual / target if target > 0 else float('inf')
            if ratio <= 0.5:
                return 'A'
            elif ratio <= 0.8:
                return 'B'
            elif ratio <= 1.0:
                return 'C'
            else:
                return 'D'
    
    def generate_benchmark_report(self):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š...")
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        grades = [result.get('performance_grade', 'D') for result in self.results.values() if 'performance_grade' in result]
        grade_scores = {'A': 4, 'B': 3, 'C': 2, 'D': 1}
        avg_score = statistics.mean([grade_scores[grade] for grade in grades]) if grades else 0
        
        if avg_score >= 3.5:
            overall_grade = 'A'
        elif avg_score >= 2.5:
            overall_grade = 'B'
        elif avg_score >= 1.5:
            overall_grade = 'C'
        else:
            overall_grade = 'D'
        
        # è®¡ç®—è¾¾æ ‡ç‡
        targets_met = [result.get('meets_target', False) for result in self.results.values() if 'meets_target' in result]
        target_achievement_rate = (sum(targets_met) / len(targets_met) * 100) if targets_met else 0
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'baseline_targets': self.baseline_targets,
            'test_results': self.results,
            'summary': {
                'overall_grade': overall_grade,
                'average_score': avg_score,
                'target_achievement_rate_percent': target_achievement_rate,
                'total_tests': len(self.results),
                'tests_passed': sum(targets_met)
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        logs_dir = project_root / "logs"
        logs_dir.mkdir(exist_ok=True)
        report_path = logs_dir / f"performance_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("âš¡ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    print("=" * 60)
    
    benchmark = PerformanceBenchmark()
    test_results = []
    
    try:
        # è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
        test_results.append(await benchmark.test_search_performance(50))
        test_results.append(await benchmark.test_concurrent_performance(8))
        test_results.append(benchmark.test_memory_efficiency(5000))
        test_results.append(benchmark.test_startup_performance())
        test_results.append(benchmark.test_configuration_performance())
        
        # ç”ŸæˆæŠ¥å‘Š
        report = benchmark.generate_benchmark_report()
        
        print("=" * 60)
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æ‘˜è¦:")
        print(f"  æ€»ä½“ç­‰çº§: {report['summary']['overall_grade']}")
        print(f"  å¹³å‡åˆ†æ•°: {report['summary']['average_score']:.2f}/4.0")
        print(f"  ç›®æ ‡è¾¾æˆç‡: {report['summary']['target_achievement_rate_percent']:.1f}%")
        print(f"  é€šè¿‡æµ‹è¯•: {report['summary']['tests_passed']}/{report['summary']['total_tests']}")
        
        if report['summary']['overall_grade'] in ['A', 'B']:
            print("ğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•è¡¨ç°ä¼˜ç§€ï¼")
            return True
        else:
            print("âš ï¸ æ€§èƒ½åŸºå‡†æµ‹è¯•éœ€è¦æ”¹è¿›")
            return False
            
    except Exception as e:
        print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
