#!/usr/bin/env python3
"""
æµ‹è¯•å¥—ä»¶è¿è¡Œå™¨
ç»Ÿä¸€æ‰§è¡Œæ‰€æœ‰ç›‘æ§å’Œæ€§èƒ½æµ‹è¯•
"""
import sys
import asyncio
import subprocess
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class TestSuiteRunner:
    """æµ‹è¯•å¥—ä»¶è¿è¡Œå™¨"""
    
    def __init__(self):
        self.results = {}
        self.start_time = None
        self.end_time = None
        
    def run_test_script(self, script_name: str, description: str) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•è„šæœ¬"""
        print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {description}")
        print("=" * 60)
        
        script_path = project_root / "scripts" / script_name
        
        if not script_path.exists():
            return {
                'status': 'failed',
                'error': f'æµ‹è¯•è„šæœ¬ä¸å­˜åœ¨: {script_path}',
                'execution_time': 0
            }
        
        start_time = time.time()
        
        try:
            # è¿è¡Œæµ‹è¯•è„šæœ¬
            result = subprocess.run(
                [sys.executable, str(script_path)],
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            execution_time = time.time() - start_time
            
            if result.returncode == 0:
                print("âœ… æµ‹è¯•é€šè¿‡")
                return {
                    'status': 'passed',
                    'execution_time': execution_time,
                    'stdout': result.stdout,
                    'stderr': result.stderr
                }
            else:
                print("âŒ æµ‹è¯•å¤±è´¥")
                print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                return {
                    'status': 'failed',
                    'execution_time': execution_time,
                    'stdout': result.stdout,
                    'stderr': result.stderr,
                    'return_code': result.returncode
                }
                
        except subprocess.TimeoutExpired:
            execution_time = time.time() - start_time
            print("â° æµ‹è¯•è¶…æ—¶")
            return {
                'status': 'timeout',
                'execution_time': execution_time,
                'error': 'æµ‹è¯•æ‰§è¡Œè¶…æ—¶'
            }
        except Exception as e:
            execution_time = time.time() - start_time
            print(f"ğŸ’¥ æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
            return {
                'status': 'error',
                'execution_time': execution_time,
                'error': str(e)
            }
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æµ‹è¯•å¥—ä»¶")
        print("=" * 80)
        
        self.start_time = datetime.now()
        
        # å®šä¹‰æµ‹è¯•å¥—ä»¶
        test_suite = [
            ('config_migration_validator.py', 'é…ç½®è¿ç§»éªŒè¯'),
            ('basic_monitoring_test.py', 'åŸºç¡€ç›‘æ§åŠŸèƒ½æµ‹è¯•'),
            ('performance_benchmark.py', 'æ€§èƒ½åŸºå‡†æµ‹è¯•'),
        ]
        
        # è¿è¡Œæ¯ä¸ªæµ‹è¯•
        for script_name, description in test_suite:
            test_result = self.run_test_script(script_name, description)
            self.results[script_name] = {
                'description': description,
                'result': test_result,
                'timestamp': datetime.now().isoformat()
            }
        
        self.end_time = datetime.now()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report()
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š...")
        print("=" * 80)
        
        total_tests = len(self.results)
        passed_tests = len([r for r in self.results.values() if r['result']['status'] == 'passed'])
        failed_tests = len([r for r in self.results.values() if r['result']['status'] == 'failed'])
        error_tests = len([r for r in self.results.values() if r['result']['status'] == 'error'])
        timeout_tests = len([r for r in self.results.values() if r['result']['status'] == 'timeout'])
        
        total_execution_time = sum(r['result']['execution_time'] for r in self.results.values())
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if passed_tests == total_tests:
            overall_status = 'ğŸ‰ å…¨éƒ¨é€šè¿‡'
            status_color = 'green'
        elif passed_tests >= total_tests * 0.8:
            overall_status = 'âš ï¸ å¤§éƒ¨åˆ†é€šè¿‡'
            status_color = 'yellow'
        else:
            overall_status = 'âŒ éœ€è¦ä¿®å¤'
            status_color = 'red'
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'test_suite_info': {
                'start_time': self.start_time.isoformat(),
                'end_time': self.end_time.isoformat(),
                'total_duration_seconds': (self.end_time - self.start_time).total_seconds(),
                'total_execution_time_seconds': total_execution_time
            },
            'summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': failed_tests,
                'error_tests': error_tests,
                'timeout_tests': timeout_tests,
                'success_rate_percent': success_rate,
                'overall_status': overall_status
            },
            'detailed_results': self.results,
            'recommendations': self._generate_recommendations()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        logs_dir = project_root / "logs"
        logs_dir.mkdir(exist_ok=True)
        report_path = logs_dir / f"test_suite_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ˜¾ç¤ºæ‘˜è¦
        print(f"ğŸ“‹ æµ‹è¯•æ‘˜è¦:")
        print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"  é€šè¿‡: {passed_tests}")
        print(f"  å¤±è´¥: {failed_tests}")
        print(f"  é”™è¯¯: {error_tests}")
        print(f"  è¶…æ—¶: {timeout_tests}")
        print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  æ€»è€—æ—¶: {total_execution_time:.2f}ç§’")
        print(f"  æ•´ä½“çŠ¶æ€: {overall_status}")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print(f"\nğŸ“ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for script_name, test_info in self.results.items():
            status = test_info['result']['status']
            execution_time = test_info['result']['execution_time']
            
            status_icon = {
                'passed': 'âœ…',
                'failed': 'âŒ',
                'error': 'ğŸ’¥',
                'timeout': 'â°'
            }.get(status, 'â“')
            
            print(f"  {status_icon} {test_info['description']}: {status} ({execution_time:.2f}s)")
            
            if status != 'passed' and 'error' in test_info['result']:
                print(f"    é”™è¯¯: {test_info['result']['error']}")
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•
        failed_tests = [name for name, info in self.results.items() 
                       if info['result']['status'] in ['failed', 'error']]
        
        if failed_tests:
            recommendations.append("ä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
            for test in failed_tests:
                recommendations.append(f"  - æ£€æŸ¥ {test} çš„é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤")
        
        # æ£€æŸ¥æ€§èƒ½é—®é¢˜
        slow_tests = [name for name, info in self.results.items() 
                     if info['result']['execution_time'] > 60]  # è¶…è¿‡1åˆ†é’Ÿ
        
        if slow_tests:
            recommendations.append("ä¼˜åŒ–æ…¢é€Ÿæµ‹è¯•çš„æ€§èƒ½")
            for test in slow_tests:
                exec_time = self.results[test]['result']['execution_time']
                recommendations.append(f"  - {test} æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({exec_time:.1f}s)")
        
        # æ£€æŸ¥è¶…æ—¶æµ‹è¯•
        timeout_tests = [name for name, info in self.results.items() 
                        if info['result']['status'] == 'timeout']
        
        if timeout_tests:
            recommendations.append("è§£å†³æµ‹è¯•è¶…æ—¶é—®é¢˜")
            for test in timeout_tests:
                recommendations.append(f"  - {test} æ‰§è¡Œè¶…æ—¶ï¼Œå¯èƒ½éœ€è¦å¢åŠ è¶…æ—¶æ—¶é—´æˆ–ä¼˜åŒ–ä»£ç ")
        
        # å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œæä¾›ä¼˜åŒ–å»ºè®®
        if not failed_tests and not timeout_tests:
            recommendations.extend([
                "æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è€ƒè™‘ä»¥ä¸‹ä¼˜åŒ–:",
                "  - æ·»åŠ æ›´å¤šè¾¹ç•Œæƒ…å†µæµ‹è¯•",
                "  - å¢åŠ è´Ÿè½½æµ‹è¯•",
                "  - è®¾ç½®æŒç»­é›†æˆ",
                "  - å®šæœŸè¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"
            ])
        
        return recommendations
    
    def run_specific_test(self, test_name: str):
        """è¿è¡Œç‰¹å®šæµ‹è¯•"""
        test_mapping = {
            'config': ('config_migration_validator.py', 'é…ç½®è¿ç§»éªŒè¯'),
            'monitoring': ('basic_monitoring_test.py', 'åŸºç¡€ç›‘æ§åŠŸèƒ½æµ‹è¯•'),
            'performance': ('performance_benchmark.py', 'æ€§èƒ½åŸºå‡†æµ‹è¯•'),
            'cache': ('cache_performance_test.py', 'ç¼“å­˜æ€§èƒ½æµ‹è¯•')
        }
        
        if test_name not in test_mapping:
            print(f"âŒ æœªçŸ¥çš„æµ‹è¯•åç§°: {test_name}")
            print(f"å¯ç”¨çš„æµ‹è¯•: {', '.join(test_mapping.keys())}")
            return False
        
        script_name, description = test_mapping[test_name]
        result = self.run_test_script(script_name, description)
        
        return result['status'] == 'passed'

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Discordæœºå™¨äººæµ‹è¯•å¥—ä»¶è¿è¡Œå™¨')
    parser.add_argument('--test', '-t', help='è¿è¡Œç‰¹å®šæµ‹è¯• (config, monitoring, performance, cache)')
    parser.add_argument('--all', '-a', action='store_true', help='è¿è¡Œæ‰€æœ‰æµ‹è¯•')
    
    args = parser.parse_args()
    
    runner = TestSuiteRunner()
    
    if args.test:
        success = runner.run_specific_test(args.test)
        sys.exit(0 if success else 1)
    elif args.all or len(sys.argv) == 1:
        runner.run_all_tests()
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡
        all_passed = all(info['result']['status'] == 'passed' 
                        for info in runner.results.values())
        sys.exit(0 if all_passed else 1)
    else:
        parser.print_help()
        sys.exit(1)

if __name__ == "__main__":
    main()
